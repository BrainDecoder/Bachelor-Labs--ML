EXERCISES: Multiple Linear Regression Exercises

a) Calculate the parameter estimates (β0, β1, β2 and σ2), in addition find the usual 95% confidence intervals for β0, β1, and β2

Introducing the data:

> D <- data.frame(
+     x1=c(0.58, 0.86, 0.29, 0.20, 0.56, 0.28, 0.08, 0.41, 0.22, 0.35,
+          0.59, 0.22, 0.26, 0.12, 0.65, 0.70, 0.30, 0.70, 0.39, 0.72,
+          0.45, 0.81, 0.04, 0.20, 0.95),
+     x2=c(0.71, 0.13, 0.79, 0.20, 0.56, 0.92, 0.01, 0.60, 0.70, 0.73,
+          0.13, 0.96, 0.27, 0.21, 0.88, 0.30, 0.15, 0.09, 0.17, 0.25,
+          0.30, 0.32, 0.82, 0.98, 0.00),
+     y=c(1.45, 1.93, 0.81, 0.61, 1.55, 0.95, 0.45, 1.14, 0.74, 0.98,
+         1.41, 0.81, 0.89, 0.68, 1.39, 1.53, 0.91, 1.49, 1.38, 1.73,
+         1.11, 1.68, 0.66, 0.69, 1.98)
+ )

Calculating the parameter estimates (β0, β1, β2 and σ2):

> lm.fit = lm(D$y∼D$x1+D$x2)
> summary(lm.fit)
Call:
lm(formula = D$y ~ D$x1 + D$x2)
Residuals:
     Min       1Q   Median       3Q      Max 
-0.15493 -0.07801 -0.02004  0.04999  0.30112 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 0.433547   0.065983   6.571 1.31e-06 ***
D$x1        1.652993   0.095245  17.355 2.53e-14 ***
D$x2        0.003945   0.074854   0.053    0.958    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Residual standard error: 0.1127 on 22 degrees of freedom
Multiple R-squared:  0.9399,	Adjusted R-squared:  0.9344 
F-statistic:   172 on 2 and 22 DF,  p-value: 3.699e-14
β0 = 0.433547, β1 = 1.652993, β2 = 0.003945, σ2 (Risidual standard error) = 0.1127

The 95% confidence intervals for β0, β1, and β2 are as follows:

> confint(lm.fit)
                 2.5 %    97.5 %
(Intercept)  0.2967067   0.5703875
D$x1         1.4554666   1.8505203
D$x2        -0.1512924   0.1591822


b) Still using confidence level α = 0.05 reduce the model if appropriate.

Model reducing means simplifying the model by eliminating insignificant terms, namely those that have a high p-value. In our regression model, the x2 predictor has a 0.958 p-value and after eliminating it the confidence intervals and the p-value are still significant for the current study. 

> lm.fit = lm(D$y∼D$x1)
> summary(lm.fit)
Call:
lm(formula = D$y ~ D$x1)
Residuals:
     Min       1Q   Median       3Q      Max 
-0.15633 -0.07633 -0.02145  0.05157  0.29994 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.43609    0.04399   9.913 9.02e-10 ***
D$x1         1.65121    0.08707  18.963 1.54e-15 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Residual standard error: 0.1102 on 23 degrees of freedom
Multiple R-squared:  0.9399,	Adjusted R-squared:  0.9373 
F-statistic: 359.6 on 1 and 23 DF,  p-value: 1.538e-15
After reduction of the model the p-value is 1.538e-15 and the confidence intervals are as follows:

> confint(lm.fit)
                2.5 %    97.5 %
(Intercept) 0.3450849 0.5270978
D$x1        1.4710842 1.8313341


c) Carry out a residual analysis to check that the model assumptions are fulfilled.

RSE, or the Residual Standard Error, is the average amount that the response will deviate from the true regression line. The smaller RSE is, the better the model fits the data. Before reduction, RSE was 0.1127 after it is 0.1102. Since it decreased, it means that the model fits better the data we work with, and reducing the model was a correct choice. R2 equals 0.9399 and is very close to 1, which means that a large proportion of the variability in the response has been explained by the regression, which again means that the model is chosen correctly. 


d) Make a plot of the fitted line and 95% confidence and prediction intervals of the line for x1 ϵ [0, 1] (it is assumed that the model was reduced above).

Drawing the plot:

> plot(D$y∼D$x1, ylim(c(0, 2.2)))
Drawing the regression line, in this case, it's black:

> abline (lm.fit)
Drawing the lower bound of the 95% confidence interval taken from the results of confint(lm.fit), red in this case:

> abline (0.3450849, 1.4710842, col ="red ")
Drawing the upper bound of the 95% confidence interval taken from the results of confint(lm.fit), also as a red line:

> abline (0.5270978, 1.8313341, col ="red ")
Calculating the prediction intervals of the line for x1 ϵ [0, 1]:

> predict (lm.fit, data.frame(x1=c(0,1)), interval ="predict")
        fit       lwr       upr
1 0.4360914 0.1905412 0.6816415
2 2.0873005 1.8335882 2.3410129
Drawing the fit line for x1 = 0:

> abline (0.4360914, 0, col ="green")
Drawing the fit line for x1 = 1:

> abline (2.0873005, 0, col ="green")
This is how the plot looks like after the above operations:



EXERCISES: MLR simulation

a) Plot the observed values of y as a function of x1 and x2. Does it seem reasonable that either x1 or x2 can describe the variation in y?

Entering the values into R:

> D <- data.frame(
+     y=c(9.29,12.67,12.42,0.38,20.77,9.52,2.38,7.46),
+     x1=c(1.00,2.00,3.00,4.00,5.00,6.00,7.00,8.00),
+     x2=c(4.00,12.00,16.00,8.00,32.00,24.00,20.00,28.00)
+ )
> y = D$y
> x1 = D$x1
> x2 = D$x2
Plotting x1:

> plot(y∼x1)
> abline(lm(y~x1))

Plotting x2:

> plot(y∼x2)
> abline(lm(y~x2))

From the 2 graphs above it's clearly seen that the 2 involved predictors have different slopes. The same can be found in the Coefficients results in the summary() function: 

> summary(lm(y∼x1+x2))
Call:
lm(formula = y ~ x1 + x2)
Residuals:
       1        2        3 
 0.96220  0.17829 -0.36698 
       4        5        6 
-1.09634 -0.34479 -0.28416 
       7        8 
 0.01784  0.93394 
Coefficients:
            Estimate Std. Error
(Intercept)  8.03253    0.67279
x1          -3.57336    0.19548
x2           0.96716    0.04887
            t value Pr(>|t|)    
(Intercept)   11.94 7.27e-05 ***
x1           -18.28 9.01e-06 ***
x2            19.79 6.08e-06 ***
---
Signif. codes:  
  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’
  0.05 ‘.’ 0.1 ‘ ’ 1
Residual standard error: 0.8205 on 5 degrees of freedom
Multiple R-squared:  0.9881,	Adjusted R-squared:  0.9834 
F-statistic:   208 on 2 and 5 DF,  p-value: 1.536e-05
The slope of x1 is -3.57336, and the slope of x2 is 0.96716. Even if this difference creates confusion, the small p-value and R2 close to 1 show that the model is strong and can be trusted. For a better understanding, we'll build the summary() function for each predictor separately. So, for x1 the results are:

> summary(lm(y∼x1))
Call:
lm(formula = y ~ x1)
Residuals:
    Min      1Q  Median      3Q     Max 
-9.2942 -3.0504  0.6933  1.8381 11.7217 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  12.1775     5.1984   2.343   0.0576 .
x1           -0.6258     1.0294  -0.608   0.5655  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Residual standard error: 6.672 on 6 degrees of freedom
Multiple R-squared:  0.05802,	Adjusted R-squared:  -0.09897 
F-statistic: 0.3696 on 1 and 6 DF,  p-value: 0.5655
and for x2 the results are:

> summary(lm(y∼x2))
Call:
lm(formula = y ~ x2)
Residuals:
   Min     1Q Median     3Q    Max 
-7.554 -5.104  1.036  4.212  7.397 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   4.2039     4.8069   0.875    0.415
x2            0.2865     0.2380   1.204    0.274
Residual standard error: 6.169 on 6 degrees of freedom
Multiple R-squared:  0.1946,	Adjusted R-squared:  0.06035 
F-statistic:  1.45 on 1 and 6 DF,  p-value: 0.2739
In both latter cases, the p-values are high and the R2 values are closer to 0 than 1. This proves that the assumption that "either x1 or x2 can describe the variation in y" is not true. For this study case, x1 and x2 should be analyzed together in order to give trustful insights and/or predictions for the available data set. 


b) Estimate the parameters for the two models

As mentioned above, for this data set the model has power only when both x1 and x2 are examined together. Therefore, the parameters are estimated based on the following function:

> summary(lm(y∼x1+x2))
Call:
lm(formula = y ~ x1 + x2)
Residuals:
       1        2        3 
 0.96220  0.17829 -0.36698 
       4        5        6 
-1.09634 -0.34479 -0.28416 
       7        8 
 0.01784  0.93394 
Coefficients:
            Estimate Std. Error
(Intercept)  8.03253    0.67279
x1          -3.57336    0.19548
x2           0.96716    0.04887
            t value Pr(>|t|)    
(Intercept)   11.94 7.27e-05 ***
x1           -18.28 9.01e-06 ***
x2            19.79 6.08e-06 ***
---
Signif. codes:  
  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’
  0.05 ‘.’ 0.1 ‘ ’ 1
Residual standard error: 0.8205 on 5 degrees of freedom
Multiple R-squared:  0.9881,	Adjusted R-squared:  0.9834 
F-statistic:   208 on 2 and 5 DF,  p-value: 1.536e-05
β0, in this case, is 8.03253, β1x1 is -3.57336, β1x2 is 0.96716 and σ2 is 0.8205.

The confidence interval can be calculated as follows:

> confint(lm(y∼x1+x2))
                 2.5 %    97.5 %
(Intercept)  6.3030657  9.762002
x1          -4.0758702   -3.070860
x2           0.8415325    1.092785
From the above results, we see that x1 is significantly different from 0 in comparison with x2 on a 95% confidence level. 



References: 
James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning with Applications in R. New York, NY: Springer.
