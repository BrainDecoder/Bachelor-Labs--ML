Creating the data frame:
> df = data.frame(x = c(0, 1, 2, 5.5, 5, 6), y = c(0, 1, 2, 5.5, 6, 5), cat = c(1, 1, 1, 2, 2, 2))

Plotting the points
> plot(df$x, df$y, col=c("blue","blue","blue","red","red","red"))

Selecting the training data set, which is the first and second columns of the data frame:
> train.df = df[c(1,2)]

Creating the test data set:
> test.df = data.frame(x = 4, y = 4)

Selecting the vector of training responses, which is the third column of the data frame:
> cl = df[c(3)]

Training the model:
> library(class)
> knn.1 = knn(train = train.df, test = test.df, cl = cl, k=1)

Printing results:
> summary(knn.1)
1   2 
0   1 
1 and 2 in the first row must be the categories from the 'cat' column in the original data frame. 1 in the second line must mean that the testing point, which is (4,4), was classified as category 2. This result is correct because the (5.5, 5.5) point is closer to (4, 4) than (2, 2). 

Changing the test data set and re-running the training:
> test.df = data.frame(x = 3.5, y = 3.5)
> knn.1 = knn(train = train.df, test = test.df, cl = cl, k=1)
> summary(knn.1)
1   2 
1   0 
This result shows that (3.5, 3.5) was classified as category 1. 

Increasing k to 3:
> knn.3 = knn(train = train.df, test = test.df, cl = cl, k=3)
> summary(knn.3)
1   2 
0   1 
When 3 nearest neighbors are taken into consideration, (3.5, 3.5) is now assigned to category 2. 

Adding test cases and re-running the training with k = 3:
> test.df = data.frame(x = c(4, 3, 5, 7), y = c(4, 3, 6, 7))
> knn.3 = knn(train = train.df, test = test.df, cl = cl, k=3)
> summary(knn.3)
1 2 
1 3
Results show that 1 case was classified as category 1 and 3 cases were classified as category 2. 

References: 
Skand, K., (2017), kNN(k-Nearest Neighbour) Algorithm in R, Retrieved from: https://rstudio-pubs-static.s3.amazonaws.com/316172_a857ca788d1441f8be1bcd1e31f0e875.html
